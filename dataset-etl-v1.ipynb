{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -2799638887458953,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ed21c7-3aa6-4004-86c6-092a258e7f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# Variables\n",
    "blob_account_name = \"streamersdata\"\n",
    "blob_container_name = \"staging\"\n",
    "blob_sas_token = \"sp=rl&st=2024-11-29T08:42:08Z&se=2024-12-06T16:42:08Z&spr=https&sv=2022-11-02&sr=c&sig=kzw1HEKoFhGoIiU1U3KO%2FkR%2BOThXS0NX71gJiJ2vR1M%3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -2799638887458953,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed5abe8-d717-4928-8b6d-2bc5cec8a592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SAS token is valid until 2024-12-06T16:42:08Z.\n"
     ]
    }
   ],
   "source": [
    "# Check if the token is still valid\n",
    "expiry_date_str = blob_sas_token.split(\"&se=\")[1].split(\"&\")[0]\n",
    "expiry_date = datetime.strptime(expiry_date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "current_date = datetime.utcnow()\n",
    "\n",
    "if current_date > expiry_date:\n",
    "    raise Exception(\"The SAS token is not valid anymore.\")\n",
    "else:\n",
    "    print(f\"The SAS token is valid until {expiry_date_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -2799638887458953,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35fbb48a-fd27-4b5a-abe8-cc409c248304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/staging has been unmounted.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/staging/amazon-prime.csv</td><td>amazon-prime.csv</td><td>6829543</td><td>1732868284000</td></tr><tr><td>dbfs:/mnt/staging/apple-tv-plus.csv</td><td>apple-tv-plus.csv</td><td>1408035</td><td>1732868281000</td></tr><tr><td>dbfs:/mnt/staging/hbo-max.csv</td><td>hbo-max.csv</td><td>1106192</td><td>1732868281000</td></tr><tr><td>dbfs:/mnt/staging/hulu.csv</td><td>hulu.csv</td><td>689237</td><td>1732868281000</td></tr><tr><td>dbfs:/mnt/staging/netflix.csv</td><td>netflix.csv</td><td>5037440</td><td>1732868284000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/staging/amazon-prime.csv",
         "amazon-prime.csv",
         6829543,
         1732868284000
        ],
        [
         "dbfs:/mnt/staging/apple-tv-plus.csv",
         "apple-tv-plus.csv",
         1408035,
         1732868281000
        ],
        [
         "dbfs:/mnt/staging/hbo-max.csv",
         "hbo-max.csv",
         1106192,
         1732868281000
        ],
        [
         "dbfs:/mnt/staging/hulu.csv",
         "hulu.csv",
         689237,
         1732868281000
        ],
        [
         "dbfs:/mnt/staging/netflix.csv",
         "netflix.csv",
         5037440,
         1732868284000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mount point\n",
    "mount_point = f\"/mnt/{blob_container_name}\"\n",
    "\n",
    "# Unmount if already mounted\n",
    "if mount_point in [mnt.mountPoint for mnt in dbutils.fs.mounts()]:\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "# Mount the Blob Storage\n",
    "dbutils.fs.mount(\n",
    "    source=f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net\",\n",
    "    mount_point=mount_point,\n",
    "    extra_configs={f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\": blob_sas_token}\n",
    ")\n",
    "\n",
    "# List files in the container\n",
    "display(dbutils.fs.ls(mount_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -2799638887458953,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a782f6-d733-4bf7-95b6-02d64ad032cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV to DataFrame function\n",
    "def load_csv_to_df(relative_path):\n",
    "    csv_file_path = f\"{mount_point}/{relative_path}\"\n",
    "    return spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file_path)\n",
    "\n",
    "# Relative paths\n",
    "amazon_relative_path = \"amazon-prime.csv\"\n",
    "apple_relative_path = \"apple-tv-plus.csv\"\n",
    "hbo_relative_path = \"hbo-max.csv\"\n",
    "hulu_relative_path = \"hulu.csv\"\n",
    "netflix_relative_path = \"netflix.csv\"\n",
    "\n",
    "# Load DataFrames\n",
    "amazon_df = load_csv_to_df(amazon_relative_path)\n",
    "apple_df = load_csv_to_df(apple_relative_path)\n",
    "hbo_df = load_csv_to_df(hbo_relative_path)\n",
    "hulu_df = load_csv_to_df(hulu_relative_path)\n",
    "netflix_df = load_csv_to_df(netflix_relative_path)\n",
    "\n",
    "# Display one of the DataFrames\n",
    "#display(amazon_df.limit(10))\n",
    "#amazon_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -2799638887458953,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e019e214-008e-4469-bd45-8ee4c8d672ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column with the platform name\n",
    "amazon_df = amazon_df.withColumn(\"platform\", lit(\"Amazon Prime\"))\n",
    "\n",
    "# Remove rows from the \"title\" column that contain null values. These are unusable in this case.\n",
    "amazon_df = amazon_df.filter(amazon_df[\"title\"].isNotNull())\n",
    "\n",
    "# Change the data types of specific columns\n",
    "amazon_df = amazon_df.withColumn(\"releaseYear\", col(\"releaseYear\").cast(\"integer\"))\n",
    "amazon_df = amazon_df.withColumn(\"imdbNumVotes\", col(\"imdbNumVotes\").cast(\"integer\"))\n",
    "amazon_df = amazon_df.withColumn(\"imdbAverageRating\", col(\"imdbAverageRating\").cast(\"float\"))\n",
    "\n",
    "# Fill null values in multiple columns\n",
    "amazon_df = amazon_df.fillna({\n",
    "    \"type\": \"Unknown\",\n",
    "    \"genres\": \"Unknown\",\n",
    "    \"releaseYear\": 9999,\n",
    "    \"imdbId\": \"Unknown\",\n",
    "    \"imdbAverageRating\": -1,\n",
    "    \"imdbNumVotes\": 0,\n",
    "    \"availableCountries\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Display the updated DataFrame\n",
    "#display(amazon_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -2799638887458953,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f6caf8-250e-4910-9b68-7c1ea90a1764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to transform DataFrame\n",
    "def transform_df(df, platform_name):\n",
    "    df = df.withColumn(\"platform\", lit(platform_name))\n",
    "    df = df.filter(df[\"title\"].isNotNull())\n",
    "    df = df.withColumn(\"releaseYear\", col(\"releaseYear\").cast(\"integer\"))\n",
    "    df = df.withColumn(\"imdbNumVotes\", col(\"imdbNumVotes\").cast(\"integer\"))\n",
    "    df = df.withColumn(\"imdbAverageRating\", col(\"imdbAverageRating\").cast(\"float\"))\n",
    "    df = df.fillna({\n",
    "        \"type\": \"Unknown\",\n",
    "        \"genres\": \"Unknown\",\n",
    "        \"releaseYear\": 9999,\n",
    "        \"imdbId\": \"Unknown\",\n",
    "        \"imdbAverageRating\": -1,\n",
    "        \"imdbNumVotes\": 0,\n",
    "        \"availableCountries\": \"Unknown\"\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Apply the transformation function to each DataFrame\n",
    "amazon_df_cleaned = transform_df(amazon_df, \"Amazon Prime\")\n",
    "apple_df_cleaned = transform_df(apple_df, \"Apple TV Plus\")\n",
    "hbo_df_cleaned = transform_df(hbo_df, \"HBO Max\")\n",
    "hulu_df_cleaned = transform_df(hulu_df, \"Hulu\")\n",
    "netflix_df_cleaned = transform_df(netflix_df, \"Netflix\")\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_df = amazon_df_cleaned.unionByName(apple_df_cleaned) \\\n",
    "                               .unionByName(hbo_df_cleaned) \\\n",
    "                               .unionByName(hulu_df_cleaned) \\\n",
    "                               .unionByName(netflix_df_cleaned)\n",
    "\n",
    "# Display one of the transformed DataFrames\n",
    "#display(amazon_df_cleaned.limit(10))\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#display(combined_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -2799638887458953,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a025e36c-4467-4e2c-8dec-75beec163945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the JDBC URL and connection properties\n",
    "jdbc_url = \"jdbc:sqlserver://streamers-sqlserver.database.windows.net:1433;database=streamers-sqldb\"\n",
    "connection_properties = {\n",
    "    \"user\": \"Thomas\",\n",
    "    \"password\": \"Gitaar%82\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Save the combined DataFrame to the SQL database\n",
    "combined_df.write.jdbc(url=jdbc_url, table=\"streaming_data\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -2799638887458953,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0daec26-31fe-47eb-aace-2c393498839b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2700231208197324>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyodbc\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msqlalchemy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_engine\n",
       "\u001B[1;32m      4\u001B[0m server \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstreamers-sqlserver.database.windows.net\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m database \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstreamers-sqldb\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sqlalchemy'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'sqlalchemy'"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2700231208197324>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyodbc\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msqlalchemy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_engine\n\u001B[1;32m      4\u001B[0m server \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstreamers-sqlserver.database.windows.net\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      5\u001B[0m database \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstreamers-sqldb\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sqlalchemy'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Connect to Azure SQL Database\n",
    "\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "server = 'streamers-sqlserver.database.windows.net'\n",
    "database = 'streamers-sqldb'\n",
    "username = 'Thomas'\n",
    "password = 'Gitaar%82'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver={driver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c42cc0f4-d2dd-4637-b507-14ece53210f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1637888490068948>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Insert into Content\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m combined_df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTitle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mType\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReleaseYear\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIMDbID\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIMDbAverageRating\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIMDbNumVotes\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mto_sql(\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mContent\u001B[39m\u001B[38;5;124m'\u001B[39m, con\u001B[38;5;241m=\u001B[39mengine, if_exists\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m )\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'combined_df' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'combined_df' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'combined_df' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-1637888490068948>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Insert into Content\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m combined_df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTitle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mType\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReleaseYear\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIMDbID\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIMDbAverageRating\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIMDbNumVotes\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mto_sql(\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mContent\u001B[39m\u001B[38;5;124m'\u001B[39m, con\u001B[38;5;241m=\u001B[39mengine, if_exists\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m      4\u001B[0m )\n",
        "\u001B[0;31mNameError\u001B[0m: name 'combined_df' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Insert into Content\n",
    "combined_df[['Title', 'Type', 'ReleaseYear', 'IMDbID', 'IMDbAverageRating', 'IMDbNumVotes']].to_sql(\n",
    "    'Content', con=engine, if_exists='append', index=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6149ef00-2b6d-4744-a284-3b3cfd617fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insert unique genres\n",
    "#unique_genres = set(genre for genres in content_df['genres'] for genre in genres.split(', '))\n",
    "#genres_df = pd.DataFrame({'GenreName': list(unique_genres)})\n",
    "#genres_df.to_sql('Genres', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# Insert unique countries\n",
    "#unique_countries = set(country for countries in content_df['availableCountries'] for country in countries.split(', '))\n",
    "#countries_df = pd.DataFrame({'CountryCode': list(unique_countries)})\n",
    "#countries_df.to_sql('Countries', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# Insert into ContentGenres\n",
    "# Use SQLAlchemy to map ContentID to GenreID\n",
    "\n",
    "# Insert into ContentCountries\n",
    "# Use SQLAlchemy to map ContentID to CountryID"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2700231208197253,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "dataset-etl-v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}