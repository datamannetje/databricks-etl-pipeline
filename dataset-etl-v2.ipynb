{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e432ca61-1e33-4ccb-ba9d-cc89a4e4a857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Libraries & variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ed21c7-3aa6-4004-86c6-092a258e7f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import explode, col, lit, split\n",
    "\n",
    "# Variables\n",
    "blob_account_name = \"streamersdata\"\n",
    "blob_container_name = \"staging\"\n",
    "blob_sas_token = \"<blob_sas_token>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a025e36c-4467-4e2c-8dec-75beec163945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the JDBC URL and connection properties\n",
    "jdbc_url = \"jdbc:sqlserver://streamers-sqlserver.database.windows.net:1433;database=streamers-sqldb\"\n",
    "connection_properties = {\n",
    "    \"user\": \"<USERNAME>\",\n",
    "    \"password\": \"<PASSWORD\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed5abe8-d717-4928-8b6d-2bc5cec8a592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SAS token is valid until 2024-12-20T17:03:16Z.\n"
     ]
    }
   ],
   "source": [
    "# Check if the token is still valid\n",
    "expiry_date_str = blob_sas_token.split(\"&se=\")[1].split(\"&\")[0]\n",
    "expiry_date = datetime.strptime(expiry_date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "current_date = datetime.utcnow()\n",
    "\n",
    "if current_date > expiry_date:\n",
    "    raise Exception(\"The SAS token is not valid anymore.\")\n",
    "else:\n",
    "    print(f\"The SAS token is valid until {expiry_date_str}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac03ce3-9f2a-4eb4-a840-04ea419fb224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# File mounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35fbb48a-fd27-4b5a-abe8-cc409c248304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/staging has been unmounted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/staging/amazon_prime.csv</td><td>amazon_prime.csv</td><td>6868017</td><td>1734100958000</td></tr><tr><td>dbfs:/mnt/staging/apple_tv.csv</td><td>apple_tv.csv</td><td>1394527</td><td>1734100958000</td></tr><tr><td>dbfs:/mnt/staging/hbo_max.csv</td><td>hbo_max.csv</td><td>1113776</td><td>1734100958000</td></tr><tr><td>dbfs:/mnt/staging/hulu.csv</td><td>hulu.csv</td><td>691846</td><td>1734100958000</td></tr><tr><td>dbfs:/mnt/staging/netflix.csv</td><td>netflix.csv</td><td>5044133</td><td>1734100959000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/staging/amazon_prime.csv",
         "amazon_prime.csv",
         6868017,
         1734100958000
        ],
        [
         "dbfs:/mnt/staging/apple_tv.csv",
         "apple_tv.csv",
         1394527,
         1734100958000
        ],
        [
         "dbfs:/mnt/staging/hbo_max.csv",
         "hbo_max.csv",
         1113776,
         1734100958000
        ],
        [
         "dbfs:/mnt/staging/hulu.csv",
         "hulu.csv",
         691846,
         1734100958000
        ],
        [
         "dbfs:/mnt/staging/netflix.csv",
         "netflix.csv",
         5044133,
         1734100959000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mount point\n",
    "mount_point = f\"/mnt/{blob_container_name}\"\n",
    "\n",
    "# Unmount if already mounted\n",
    "if mount_point in [mnt.mountPoint for mnt in dbutils.fs.mounts()]:\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "# Mount the Blob Storage\n",
    "dbutils.fs.mount(\n",
    "    source=f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net\",\n",
    "    mount_point=mount_point,\n",
    "    extra_configs={f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\": blob_sas_token}\n",
    ")\n",
    "\n",
    "# List files in the container\n",
    "display(dbutils.fs.ls(mount_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9755d1-0636-48b7-a74b-5dff12fcc77c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a782f6-d733-4bf7-95b6-02d64ad032cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV to DataFrame function\n",
    "def load_csv_to_df(relative_path):\n",
    "    csv_file_path = f\"{mount_point}/{relative_path}\"\n",
    "    return spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file_path)\n",
    "\n",
    "# Relative paths\n",
    "amazon_relative_path = \"amazon_prime.csv\"\n",
    "apple_relative_path = \"apple_tv.csv\"\n",
    "hbo_relative_path = \"hbo_max.csv\"\n",
    "hulu_relative_path = \"hulu.csv\"\n",
    "netflix_relative_path = \"netflix.csv\"\n",
    "\n",
    "# Load DataFrames\n",
    "amazon_df = load_csv_to_df(amazon_relative_path)\n",
    "apple_df = load_csv_to_df(apple_relative_path)\n",
    "hbo_df = load_csv_to_df(hbo_relative_path)\n",
    "hulu_df = load_csv_to_df(hulu_relative_path)\n",
    "netflix_df = load_csv_to_df(netflix_relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0132d23f-f719-4198-a5e8-c208f2b74963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dataframe cleaning & transformation\n",
    "The dataframes need to be transformed for easy processing.\n",
    "1. I remove all the records without data. If the columns title, imdbId, imdbAverageRating or imdbNumVotes contain Null the row is deleted.\n",
    "2. The data in the dataframes are in string format. So I change them to integer or float if needed.\n",
    "3. Null's in the releaseYear column are replaced with 9999\n",
    "4. A new column is added with the platform name.\n",
    "5. Last step is combining. Luckely the five different CSV files follow the same template. So combining them is not to difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f6caf8-250e-4910-9b68-7c1ea90a1764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to transform DataFrame\n",
    "def transform_df(df, platform_name):\n",
    "    df = df.withColumn(\"platform\", lit(platform_name))\n",
    "    df = df.filter(\n",
    "        col(\"title\").isNotNull() &\n",
    "        col(\"imdbId\").isNotNull() &\n",
    "        col(\"imdbAverageRating\").isNotNull() &\n",
    "        col(\"imdbNumVotes\").isNotNull()\n",
    "    )\n",
    "    df = df.withColumn(\"releaseYear\", col(\"releaseYear\").cast(\"integer\"))\n",
    "    df = df.withColumn(\"imdbNumVotes\", col(\"imdbNumVotes\").cast(\"integer\"))\n",
    "    df = df.withColumn(\"imdbAverageRating\", col(\"imdbAverageRating\").cast(\"float\"))\n",
    "    df = df.fillna({\n",
    "        \"type\": \"Unknown\",\n",
    "        \"genres\": \"Unknown\",\n",
    "        \"releaseYear\": 9999,\n",
    "        \"availableCountries\": \"Unknown\"\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Apply the transformation function to each DataFrame\n",
    "amazon_df_cleaned = transform_df(amazon_df, \"Amazon Prime\")\n",
    "apple_df_cleaned = transform_df(apple_df, \"Apple TV Plus\")\n",
    "hbo_df_cleaned = transform_df(hbo_df, \"HBO Max\")\n",
    "hulu_df_cleaned = transform_df(hulu_df, \"Hulu\")\n",
    "netflix_df_cleaned = transform_df(netflix_df, \"Netflix\")\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_df = amazon_df_cleaned.unionByName(apple_df_cleaned) \\\n",
    "                               .unionByName(hbo_df_cleaned) \\\n",
    "                               .unionByName(hulu_df_cleaned) \\\n",
    "                               .unionByName(netflix_df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9559a9d-485e-4056-ad46-c287a4c02ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Specific DataFrames: Content, Genre, Country, Platform\n",
    "Each specific dataframe contains the unique records from the combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ce3255-aaa3-4719-85c6-881f0acb8f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to create dataframes with unique records per type\n",
    "def extract_unique_data(column_name, alias_name):\n",
    "  # Explode into multiple rows\n",
    "  df = combined_df.select(\n",
    "      col(\"title\"),\n",
    "      explode(split(col(column_name), \",\\\\s*\")).alias(alias_name)\n",
    "  )\n",
    "\n",
    "  # Extract unique records\n",
    "  unique_df = df.select(alias_name).distinct()\n",
    "\n",
    "  return unique_df\n",
    "\n",
    "# Dataframes with unique records\n",
    "unique_countries_df  = extract_unique_data(\"availableCountries\", \"CountryCode\")\n",
    "unique_genres_df = extract_unique_data(\"genres\", \"GenreName\")\n",
    "unique_platform_df = extract_unique_data(\"platform\", \"platformName\")\n",
    "\n",
    "# Creat content specific dataframe\n",
    "content_df = combined_df.select(\"title\", \"type\", \"releaseYear\", \"imdbId\", \"imdbAverageRating\", \"imdbNumVotes\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eaf63b9-e70c-47a4-8df2-2b9f053128cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Loading dataframes to SQL database\n",
    "\n",
    "To make sure only new data is loaded on to the server the exsisting data is first put in a dataframe and joined with the new dataframe. This is done for the content, genres and country dataframes.\n",
    "Next tables based on a join are created: ContentGenres, ContentCountries and ContentPlatform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43dc3c8c-25d1-489f-bd9d-8bb5301c990c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to write data to SQL database\n",
    "\n",
    "def load_write_to_sql(df, table_name, column_name):\n",
    "  current_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=table_name,\n",
    "    properties=connection_properties\n",
    "  )\n",
    "  \n",
    "  new_records_df = df.join(\n",
    "    current_df, on=column_name, how=\"left_anti\"\n",
    "  )\n",
    "\n",
    "  new_records_df.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=table_name,\n",
    "    mode=\"append\",\n",
    "    properties=connection_properties\n",
    "  )\n",
    "\n",
    "load_write_to_sql(content_df, \"Content\", \"imdbID\") # Write data to \"Content\" table\n",
    "load_write_to_sql(unique_countries_df, \"Countries\", \"CountryCode\") # Write data to \"Countries\" table\n",
    "load_write_to_sql(unique_genres_df, \"Genres\", \"GenreName\") # Write data to \"Genrename\" table\n",
    "load_write_to_sql(unique_platform_df, \"Platforms\", \"platformName\") # Write data to \"Platforms\" table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2226f39b-675e-4300-8cb7-70ac6684b1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to combine data from multiple tables and write to SQL database\n",
    "\n",
    "def combine_write_to_sql(column_name, alias_name, table_name, sql_query, combi_table_name):\n",
    "  \n",
    "  # Load exsisting table from server\n",
    "  def load_current_df(table):\n",
    "    current_df = spark.read.jdbc(\n",
    "      url=jdbc_url,\n",
    "      table=table,\n",
    "      properties=connection_properties\n",
    "    )\n",
    "    return current_df\n",
    "  \n",
    "  existing_content_df = load_current_df(\"Content\") \n",
    "  existing_table_df = load_current_df(table_name)\n",
    "\n",
    "  # Explode records into multiple rows and select the required columns\n",
    "  content_combi_df = combined_df.select(\n",
    "    col(\"imdbId\").alias(\"contentID\"), \n",
    "    explode(split(col(column_name), \",\")).alias(alias_name)\n",
    "  ).distinct()\n",
    "\n",
    "  # Create or replace temporary views\n",
    "  content_combi_df.createOrReplaceTempView(\"temp_content_combi\")\n",
    "  existing_content_df.createOrReplaceTempView(\"Content\")\n",
    "  existing_table_df.createOrReplaceTempView(table_name)\n",
    "\n",
    "  # Perform the join and select the required columns\n",
    "  content_combi_result_df = spark.sql(sql_query)\n",
    "\n",
    "  # Write the result DataFrame to the SQL Server table\n",
    "  content_combi_result_df.write.jdbc(\n",
    "      url=jdbc_url,\n",
    "      table=combi_table_name,\n",
    "      mode=\"overwrite\",\n",
    "      properties=connection_properties\n",
    "  )\n",
    "\n",
    "country_sql_query = \"\"\"\n",
    "SELECT c.contentID, co.countryID\n",
    "FROM temp_content_combi cc\n",
    "JOIN Content c ON c.imdbId = cc.contentID\n",
    "JOIN Countries co ON co.countryCode = cc.countryCode\n",
    "\"\"\"\n",
    "\n",
    "genre_sql_query = \"\"\"\n",
    "SELECT c.contentID, g.genreID\n",
    "FROM temp_content_combi cg\n",
    "JOIN Content c ON c.imdbId = cg.contentID\n",
    "JOIN Genres g ON g.genreName = cg.genreName\n",
    "\"\"\"\n",
    "\n",
    "platform_sql_query = \"\"\"\n",
    "SELECT c.contentID, p.platformID\n",
    "FROM temp_content_combi cp\n",
    "JOIN Content c ON c.imdbId = cp.contentID\n",
    "JOIN Platforms p ON p.platformName = cp.platformName\n",
    "\"\"\"\n",
    "\n",
    "combine_write_to_sql(\"availableCountries\", \"countryCode\", \"Countries\", country_sql_query, \"ContentCountries\")\n",
    "combine_write_to_sql(\"genres\", \"genreName\", \"Genres\", genre_sql_query, \"ContentGenres\")\n",
    "combine_write_to_sql(\"platform\", \"platformName\", \"Platforms\", platform_sql_query, \"ContentPlatform\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2700231208197253,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "dataset-etl-v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
